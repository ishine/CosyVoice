# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)
#               Jing Du (thuduj12@163.com)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os.path
import time
from typing import Dict, Optional, Callable, List, Generator, Tuple, Union
import torch
from torch import nn
import torch.nn.functional as F
from transformers import Qwen2ForCausalLM, Qwen2Config
from torchmetrics.classification import MulticlassAccuracy
from torch.nn.utils.rnn import pad_sequence, unpad_sequence
from cosyvoice.llm.decoder import ARDecoder
from cosyvoice.utils.losses import FocalLoss
from cosyvoice.utils.common import get_delay_pattern_codec, revert_delay_pattern_codec
from cosyvoice.utils.common import IGNORE_ID, th_accuracy
from cosyvoice.transformer.label_smoothing_loss import LabelSmoothingLoss
from cosyvoice.utils.mask import make_pad_mask, add_optional_chunk_mask
from cosyvoice.transformer.decoder_layer import DecoderLayer
from cosyvoice.transformer.attention import MultiHeadedAttention
from cosyvoice.transformer.positionwise_feed_forward import PositionwiseFeedForward
import logging, random

import signal, sys, atexit, requests, json

logger = logging.getLogger(__name__)
from torch.autograd import Function


# ---------- GRL ----------
class GradReverse(Function):
    @staticmethod
    def forward(ctx, x, lamb):
        ctx.lamb = lamb
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad):
        return -ctx.lamb * grad, None

def grad_reverse(x, lamb=1.0):
    return GradReverse.apply(x, lamb)

class SpeakerAdapter(nn.Module):
    def __init__(self, dim, bottleneck=128):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(dim, bottleneck),
            nn.ReLU(),   # 或者 GELU
            nn.Linear(bottleneck, dim)
        )

    def forward(self, x):
        return x + self.adapter(x)


class TransformerLM(torch.nn.Module):
    def __init__(
            self,
            text_encoder_input_size: int,
            llm_input_size: int,
            llm_output_size: int,
            text_token_size: int,
            speech_token_size: int,
            text_encoder: torch.nn.Module,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            spk_embed_dim: int = 192,
    ):
        super().__init__()
        self.llm_input_size = llm_input_size
        self.speech_token_size = speech_token_size
        # 1. build text token inputs related modules
        self.text_embedding = torch.nn.Embedding(text_token_size, text_encoder_input_size)
        self.text_encoder = text_encoder
        self.text_encoder_affine_layer = nn.Linear(
            self.text_encoder.output_size(),
            llm_input_size
        )

        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)
        self.llm = llm
        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 1)
        self.criterion_ce = LabelSmoothingLoss(
            size=speech_token_size + 1,
            padding_idx=IGNORE_ID,
            smoothing=lsm_weight,
            normalize_length=length_normalized_loss,
        )

        # 3. [Optional] build speech token related modules
        self.speech_embedding = torch.nn.Embedding(speech_token_size, llm_input_size)
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, llm_input_size)

        # 4. sampling method
        self.sampling = sampling

    def encode(
            self,
            text: torch.Tensor,
            text_lengths: torch.Tensor,
    ):
        encoder_out, encoder_mask = self.text_encoder(text, text_lengths, decoding_chunk_size=1, num_decoding_left_chunks=-1)
        encoder_out_lens = encoder_mask.squeeze(1).sum(1)
        encoder_out = self.text_encoder_affine_layer(encoder_out)
        return encoder_out, encoder_out_lens

    def pad_unpad_sequence(self, sos_eos_emb, embedding, text_token, text_token_len, task_id_emb, speech_token, speech_token_len):
        text_token = unpad_sequence(text_token, text_token_len.cpu(), batch_first=True)
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)
        lm_input = [torch.concat([sos_eos_emb.squeeze(dim=0), embedding[i], text_token[i], task_id_emb.squeeze(dim=0), speech_token[i]], dim=0)
                    for i in range(len(text_token))]
        lm_input_len = torch.tensor([i.size(0) for i in lm_input], dtype=torch.int32)
        lm_input = pad_sequence(lm_input, batch_first=True, padding_value=IGNORE_ID)
        return lm_input, lm_input_len

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Args:
            text: (B, L, D)
            text_lengths: (B,)
            audio: (B, T, N) or (B, T)
            audio_lengths: (B,)
        """
        text_token = batch['text_token'].to(device)
        text_token_len = batch['text_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        embedding = batch['embedding'].to(device)

        # 1. prepare llm_target
        lm_target = [torch.tensor([IGNORE_ID] * (2 + text_token_len[i]) + speech_token[i, :speech_token_len[i]].tolist() +
                                  [self.speech_token_size]) for i in range(text_token.size(0))]
        lm_target = pad_sequence(lm_target, batch_first=True, padding_value=IGNORE_ID).to(device)

        # 1. encode text_token
        text_token = self.text_embedding(text_token)
        text_token, text_token_len = self.encode(text_token, text_token_len)

        # 2. embedding projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)
        embedding = embedding.unsqueeze(1)

        # 3. eos and task_id
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)

        # 4. encode speech_token
        speech_token = self.speech_embedding(speech_token)

        # 5. unpad and pad
        lm_input, lm_input_len = self.pad_unpad_sequence(sos_eos_emb, embedding, text_token, text_token_len,
                                                         task_id_emb, speech_token, speech_token_len)

        # 6. run lm forward
        lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
        logits = self.llm_decoder(lm_output)
        loss = self.criterion_ce(logits, lm_target)
        acc = th_accuracy(logits.view(-1, self.speech_token_size + 1), lm_target, ignore_label=IGNORE_ID)
        return {'loss': loss, 'acc': acc}

    def sampling_ids(
            self,
            weighted_scores: torch.Tensor,
            decoded_tokens: List,
            sampling: int,
            ignore_eos: bool = True,
    ):
        top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
        # while True:
        #     top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
        #     if (not ignore_eos) or (self.speech_token_size not in top_ids):
        #         break
        return top_ids

    @torch.inference_mode()
    def inference(
            self,
            text: torch.Tensor,
            text_len: torch.Tensor,
            prompt_text: torch.Tensor,
            prompt_text_len: torch.Tensor,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
    ) -> Generator[torch.Tensor, None, None]:
        if self.fp16 is True:
            embedding = embedding.half()

        device = text.device
        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        text = self.text_embedding(text)

        # 1. encode text
        text, text_len = self.encode(text, text_len)

        # 2. encode embedding
        if embedding.shape[0] != 0:
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            embedding = embedding.unsqueeze(dim=1)
        else:
            embedding = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device).to(text.dtype)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)
        lm_input = torch.concat([sos_eos_emb, embedding, text, task_id_emb, prompt_speech_token_emb], dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        out_tokens = []
        offset = 0
        att_cache, cnn_cache = torch.zeros((0, 0, 0, 0), device=lm_input.device), torch.zeros((0, 0, 0, 0), device=lm_input.device)
        for i in range(max_len):
            y_pred, att_cache, cnn_cache = self.llm.forward_chunk(lm_input, offset=offset, required_cache_size=-1,
                                                                  att_cache=att_cache, cnn_cache=cnn_cache,
                                                                  att_mask=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]),
                                                                                                 device=lm_input.device)).to(torch.bool))
            logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
            # force continue decode first token
            if i == 0:
                logp[:, self.speech_token_size] = -float('inf')
            top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()
            if top_ids == self.speech_token_size:
                break
            # in stream mode, yield token one by one
            yield top_ids
            out_tokens.append(top_ids)
            offset += lm_input.size(1)
            lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)


class TransformerLM_Phoneme(torch.nn.Module):
    """
    input with Phoneme, Tones, Languages, Prosodys
    """
    def __init__(
            self,
            text_encoder_input_size: int,
            llm_input_size: int,
            llm_output_size: int,
            text_token_size: int,
            text_token_dim: int,
            text_tone_size: int,
            text_tone_dim: int,
            text_lang_size: int,
            text_lang_dim: int,
            text_prsd_size: int,
            text_prsd_dim: int,
            speech_token_size: int,
            text_encoder: torch.nn.Module,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            spk_embed_dim: int = 192,
            use_frontend_prsd: bool = True,
            use_pause_label: bool=True,
    ):
        super().__init__()
        self.llm_input_size = llm_input_size
        self.speech_token_size = speech_token_size
        # 1. build text token inputs related modules
        assert(text_token_dim+text_tone_dim+text_lang_dim+text_prsd_dim)==text_encoder_input_size
        # self.text_embedding = torch.nn.Embedding(text_token_size, text_token_dim)
        # self.tone_embedding = torch.nn.Embedding(text_tone_size, text_tone_dim)
        # self.lang_embedding = torch.nn.Embedding(text_lang_size, text_lang_dim)
        # self.prsd_embedding = torch.nn.Embedding(text_prsd_size, text_prsd_dim)
        self.text_embedding = nn.ModuleList([
            torch.nn.Embedding(text_token_size, text_token_dim),
            torch.nn.Embedding(text_tone_size, text_tone_dim),
            torch.nn.Embedding(text_lang_size, text_lang_dim),
            torch.nn.Embedding(text_prsd_size, text_prsd_dim)
        ])
        self.use_frontend_prsd = use_frontend_prsd
        self.use_pause_label = use_pause_label
        logger.info(f"llm use frontend prosody: {use_frontend_prsd}, use pause label: {use_pause_label}")

        self.text_encoder = text_encoder
        self.text_encoder_affine_layer = nn.Linear(
            self.text_encoder.output_size(),
            llm_input_size
        )

        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)
        self.llm = llm
        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 1)
        self.criterion_ce = LabelSmoothingLoss(
            size=speech_token_size + 1,
            padding_idx=IGNORE_ID,
            smoothing=lsm_weight,
            normalize_length=length_normalized_loss,
        )

        # 3. [Optional] build speech token related modules
        self.speech_embedding = torch.nn.Embedding(speech_token_size, llm_input_size)
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, llm_input_size)

        # 4. sampling method
        self.sampling = sampling

    def encode(
            self,
            text: torch.Tensor,
            text_lengths: torch.Tensor,
    ):
        encoder_out, encoder_mask = self.text_encoder(text, text_lengths, decoding_chunk_size=1, num_decoding_left_chunks=-1)
        encoder_out_lens = encoder_mask.squeeze(1).sum(1)
        encoder_out = self.text_encoder_affine_layer(encoder_out)
        return encoder_out, encoder_out_lens

    def pad_unpad_sequence(self, sos_eos_emb, embedding, text_token, text_token_len, task_id_emb, speech_token, speech_token_len):
        text_token = unpad_sequence(text_token, text_token_len.cpu(), batch_first=True)
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)
        lm_input = [torch.concat([sos_eos_emb.squeeze(dim=0), embedding[i], text_token[i], task_id_emb.squeeze(dim=0), speech_token[i]], dim=0)
                    for i in range(len(text_token))]
        lm_input_len = torch.tensor([i.size(0) for i in lm_input], dtype=torch.int32)
        lm_input = pad_sequence(lm_input, batch_first=True, padding_value=IGNORE_ID)
        return lm_input, lm_input_len

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Args:
            text: (B, L, D)
            text_lengths: (B,)
            audio: (B, T, N) or (B, T)
            audio_lengths: (B,)
        """
        text_token = batch['pho_token'].to(device)
        # text_tone = batch['text_tone'].to(device)
        # text_lang = batch['text_lang'].to(device)
        # text_prsd = batch['text_prsd'].to(device)

        text_token_len = batch['pho_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        embedding = batch['embedding'].to(device)

        # 1. prepare llm_target
        lm_target = [torch.tensor([IGNORE_ID] * (2 + text_token_len[i]) + speech_token[i, :speech_token_len[i]].tolist() +
                                  [self.speech_token_size]) for i in range(text_token.size(0))]
        lm_target = pad_sequence(lm_target, batch_first=True, padding_value=IGNORE_ID).to(device)

        # 1. encode text_token
        # pho_embed = self.text_embedding(text_token)
        # tone_embed = self.tone_embedding(text_tone)
        # lang_embed = self.lang_embedding(text_lang)
        # prsd_embed = self.prsd_embedding(text_prsd)
        # text_token = torch.cat([pho_embed,tone_embed,lang_embed,prsd_embed], dim=-1)
        text_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](text_token[:, :, i])
            if not self.use_frontend_prsd and i==3:
                embed *= 0.0
            text_embed_list.append(embed)
        text_token = torch.cat(text_embed_list, dim=-1)

        text_token, text_token_len = self.encode(text_token, text_token_len)

        # 2. embedding projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)
        embedding = embedding.unsqueeze(1)

        # 3. eos and task_id
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)

        # 4. encode speech_token
        speech_token = self.speech_embedding(speech_token)

        # 5. unpad and pad
        lm_input, lm_input_len = self.pad_unpad_sequence(sos_eos_emb, embedding, text_token, text_token_len,
                                                         task_id_emb, speech_token, speech_token_len)

        # 6. run lm forward
        lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
        logits = self.llm_decoder(lm_output)
        loss = self.criterion_ce(logits, lm_target)
        acc = th_accuracy(logits.view(-1, self.speech_token_size + 1), lm_target, ignore_label=IGNORE_ID)
        return {'loss': loss, 'acc': acc}

    def sampling_ids(
            self,
            weighted_scores: torch.Tensor,
            decoded_tokens: List,
            sampling: int,
            ignore_eos: bool = True,
    ):

        top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
        # while True:
        #     top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
        #     if (not ignore_eos) or (self.speech_token_size not in top_ids):
        #         break
        return top_ids

    @torch.inference_mode()
    def inference(
            self,
            text: torch.Tensor,
            text_len: torch.Tensor,
            prompt_text: torch.Tensor,
            prompt_text_len: torch.Tensor,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
    ) -> Generator[torch.Tensor, None, None]:
        device = text.device
        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        # text = self.text_embedding(text)
        text_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](text[:, :, i])
            if not self.use_frontend_prsd and i==3:
                embed *= 0.0
            text_embed_list.append(embed)
        text = torch.cat(text_embed_list, dim=-1)

        # 1. encode text
        text, text_len = self.encode(text, text_len)

        # 2. encode embedding
        if embedding.shape[0] != 0:
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            embedding = embedding.unsqueeze(dim=1)
        else:
            embedding = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)
        lm_input = torch.concat([sos_eos_emb, embedding, text, task_id_emb, prompt_speech_token_emb], dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        out_tokens = []
        offset = 0
        att_cache, cnn_cache = torch.zeros((0, 0, 0, 0), device=lm_input.device), torch.zeros((0, 0, 0, 0), device=lm_input.device)
        for i in range(max_len):
            y_pred, att_cache, cnn_cache = self.llm.forward_chunk(lm_input, offset=offset, required_cache_size=-1,
                                                                  att_cache=att_cache, cnn_cache=cnn_cache,
                                                                  att_mask=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]),
                                                                                                 device=lm_input.device)).to(torch.bool))
            logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
            # force continue decode first token
            if i == 0:
                logp[:, self.speech_token_size] = -float('inf')
            top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()
            if top_ids == self.speech_token_size:
                break
            # in stream mode, yield token one by one
            yield top_ids
            out_tokens.append(top_ids)
            offset += lm_input.size(1)
            lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)


class Qwen2Encoder(torch.nn.Module):
    def __init__(self, pretrain_path):
        super().__init__()
        if os.path.exists(f"{pretrain_path}/model.safetensors"):
            logger.info(f"Load Pretrained {pretrain_path}/model.safetensors")
            self.model = Qwen2ForCausalLM.from_pretrained(pretrain_path)
        else:
            config_dict = json.load(open(f"{pretrain_path}/config.json"))
            config = Qwen2Config(**config_dict)
            self.model = Qwen2ForCausalLM(config)
    def forward(self, xs: torch.Tensor, xs_lens: torch.Tensor):
        T = xs.size(1)
        masks = ~make_pad_mask(xs_lens, T)
        outs = self.model(
            inputs_embeds=xs,
            attention_mask=masks,
            output_hidden_states=True,
            return_dict=True,
        )
        return outs.hidden_states[-1], masks.unsqueeze(1)

    def forward_one_step(self, xs, masks, cache=None):
        input_masks = masks[:, -1, :]
        outs = self.model(
            inputs_embeds=xs,
            attention_mask=input_masks,
            output_hidden_states=True,
            return_dict=True,
            use_cache=True,
            past_key_values=cache,
        )
        xs = outs.hidden_states[-1]
        new_cache = outs.past_key_values
        return xs, new_cache


class Qwen2LM(TransformerLM):
    def __init__(
            self,
            llm_input_size: int,
            llm_output_size: int,
            speech_token_size: int,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            mix_ratio: List[int] = [5, 15],
            use_embedding: bool = False,
            spk_embed_dim: int = 512,
            qwen_sglang_config: dict = None,  # sglang/vllm加速配置
            vllm_sample_params: dict = None,  # vllm采样参数配置
            emotion_num: int = 0,
            non_emotional_label: int = -1,  # 非多情感数据标签
    ):
        torch.nn.Module.__init__(self)
        self.use_frontend_prsd = False
        self.use_pause_label = False
        self.use_embedding = use_embedding
        self.emotion_num = emotion_num
        self.non_emotional_label = non_emotional_label
        logger.info(f"use_embedding: {use_embedding}, emotion_num: {emotion_num}, non_emotional_label: {non_emotional_label}")
        if self.emotion_num > 0:
            self.spk_adapter = SpeakerAdapter(dim=llm_input_size, bottleneck=256)
            num_emotions = max(1, self.emotion_num)  # 避免为0
            self.emo_adversary = nn.Sequential(
                nn.Linear(llm_input_size, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, num_emotions)
            )
        self.adv_weight = 1.0  # 对抗强度
        self.preserve_weight = 1.0  # 保持音色相似
        self.grl_lambda = 1.0  # GRL 系数

        self.llm_input_size = llm_input_size
        self.llm_output_size = llm_output_size
        self.speech_token_size = speech_token_size
        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.fill_token = 2

        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)
        self.llm = llm
        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 3)
        self.criterion_ce = LabelSmoothingLoss(
            size=speech_token_size + 3,
            padding_idx=IGNORE_ID,
            smoothing=lsm_weight,
            normalize_length=length_normalized_loss,
        )

        # 3. [Optional] build speech token related modules
        self.speech_embedding = torch.nn.Embedding(speech_token_size + 3, llm_input_size)
        if self.use_embedding:
            self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, llm_input_size)

        # 4. sampling method
        self.sampling = sampling
        self.mix_ratio = mix_ratio

        # 5. vllm related
        self.stop_token_ids = [speech_token_size + i for i in range(3)]
        self.vllm_output_queue = {}
        self.use_vllm = (qwen_sglang_config is not None)
        self.vllm_sample_params = vllm_sample_params
        logger.info(f"vllm sampling params: {vllm_sample_params}")
        if self.use_vllm and 0==1:
            os.environ["VLLM_ATTENTION_BACKEND"] = "FLASHINFER"  # 默认后端为Flash-Attn, 改为FlashInfer
            python_bin_dir = os.path.dirname(sys.executable)
            custom_env = os.environ.copy()
            custom_env["PATH"] = f"{python_bin_dir}:{custom_env['PATH']}"
            model_path = qwen_sglang_config['model_path']
            self.base_url = qwen_sglang_config['base_url']  # 直接在同一个进程中启动，此参数不需要了
            mem_fraction = qwen_sglang_config['mem_ratio']

            from vllm import ModelRegistry
            from cosyvoice.vllm.cosyvoice2 import CosyVoice2ForCausalLM
            ModelRegistry.register_model("CosyVoice2ForCausalLM", CosyVoice2ForCausalLM)
            from vllm import EngineArgs, LLMEngine
            engine_args = EngineArgs(model=model_path,
                                     skip_tokenizer_init=True,
                                     enable_prompt_embeds=True,
                                     disable_custom_all_reduce=True,
                                     gpu_memory_utilization=float(mem_fraction))
            self.vllm = LLMEngine.from_engine_args(engine_args)
            del self.llm.model.model.layers

    def prepare_lm_input_target(self, text_token, text_token_emb, text_token_len, speech_token,
                                speech_token_emb, speech_token_len, embedding=None):
        lm_target, lm_input = [], []
        text_token = unpad_sequence(text_token, text_token_len.cpu(), batch_first=True)
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)
        text_token_emb = unpad_sequence(text_token_emb, text_token_len.cpu(), batch_first=True)
        speech_token_emb = unpad_sequence(speech_token_emb, speech_token_len.cpu(), batch_first=True)
        for i in range(len(text_token)):
            # bistream sequence
            if random.random() < 0.5 and speech_token_len[i] / text_token_len[i] > self.mix_ratio[1] / self.mix_ratio[0]:
                this_lm_target, this_lm_input = [], []
                this_lm_target.append(IGNORE_ID)
                this_lm_input.append(self.llm_embedding.weight[self.sos_eos].reshape(1, -1))
                for j in range(((text_token_len[i] + 1) / self.mix_ratio[0]).ceil().int().item()):
                    this_text_token = text_token[i][j * self.mix_ratio[0]: (j + 1) * self.mix_ratio[0]].tolist()
                    this_speech_token = speech_token[i][j * self.mix_ratio[1]: (j + 1) * self.mix_ratio[1]].tolist()
                    if len(this_text_token) == self.mix_ratio[0]:
                        assert len(this_speech_token) == self.mix_ratio[1]
                        this_lm_target += [IGNORE_ID] * (self.mix_ratio[0] - 1)
                        if embedding is not None:
                            this_lm_target += [IGNORE_ID]
                            this_lm_input.append(embedding[i])
                        this_lm_target += this_speech_token
                        this_lm_target.append(self.speech_token_size + 2)
                        this_lm_input.append(text_token_emb[i][j * self.mix_ratio[0]: (j + 1) * self.mix_ratio[0]])
                        this_lm_input.append(speech_token_emb[i][j * self.mix_ratio[1]: (j + 1) * self.mix_ratio[1]])
                    else:
                        this_lm_target += [IGNORE_ID] * len(this_text_token)
                        if embedding is not None:
                            this_lm_target += [IGNORE_ID]
                            this_lm_input.append(embedding[i])
                        this_lm_target += speech_token[i][j * self.mix_ratio[1]:].tolist()
                        this_lm_target.append(self.speech_token_size)
                        this_lm_input.append(text_token_emb[i][j * self.mix_ratio[0]:])
                        this_lm_input.append(self.llm_embedding.weight[self.task_id].reshape(1, -1))
                        this_lm_input.append(speech_token_emb[i][j * self.mix_ratio[1]:])
                this_lm_target, this_lm_input = torch.tensor(this_lm_target), torch.concat(this_lm_input, dim=0)
            # unistream sequence
            else:
                if embedding is None:
                    this_lm_target = torch.tensor(
                        [IGNORE_ID] * (1 + text_token_len[i]) + speech_token[
                            i].tolist() + [self.speech_token_size])
                    this_lm_input = torch.concat(
                        [self.llm_embedding.weight[self.sos_eos].reshape(1, -1),
                         text_token_emb[i],
                         self.llm_embedding.weight[self.task_id].reshape(1, -1),
                         speech_token_emb[i]], dim=0)
                else:
                    this_lm_target = torch.tensor(
                        [IGNORE_ID] * (2 + text_token_len[i]) + speech_token[
                            i].tolist() + [self.speech_token_size])
                    this_lm_input = torch.concat(
                        [self.llm_embedding.weight[self.sos_eos].reshape(1, -1),
                         embedding[i], text_token_emb[i],
                         self.llm_embedding.weight[self.task_id].reshape(1, -1),
                         speech_token_emb[i]], dim=0)
            lm_target.append(this_lm_target)
            lm_input.append(this_lm_input)
        lm_input_len = torch.tensor([i.size(0) for i in lm_input], dtype=torch.int32)
        lm_input = pad_sequence(lm_input, batch_first=True, padding_value=IGNORE_ID)
        lm_target = pad_sequence(lm_target, batch_first=True, padding_value=IGNORE_ID)
        return lm_target, lm_input, lm_input_len

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Args:
            text: (B, L, D)
            text_lengths: (B,)
            audio: (B, T, N) or (B, T)
            audio_lengths: (B,)
        """
        text_token = batch['text_token'].to(device)
        text_token_len = batch['text_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        if self.use_embedding:
            embedding = batch['embedding'].to(device)
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            if batch.get('emos', None) is not None and self.emotion_num > 0:
                if self.non_emotional_label == 0:
                    emotion_lab = [i if i != -1 else 0 for i in batch['emos']]  # 把-1当做0
                else:
                    emotion_lab = batch['emos']
                emotion_lab_tensor = torch.tensor(emotion_lab, dtype=torch.long, device=device)

                s_orig = embedding.detach()
                s_hat = embedding.clone()
                # ---------- adversarial loss: only on labeled samples ----------
                labeled_mask = (emotion_lab_tensor >= 0)  # (B,)
                if labeled_mask.any():
                    s_hat[labeled_mask] = self.spk_adapter(s_hat[labeled_mask])
                    s_out_labeled = s_hat[labeled_mask]  # (B_lab, D)
                    # apply GRL before classifier
                    s_adv = grad_reverse(s_out_labeled, self.grl_lambda)
                    pred = self.emo_adversary(s_adv)  # (B_lab, num_emotions)
                    emo_labels_labeled = emotion_lab_tensor[labeled_mask]  # (B_lab,)
                    adv_loss = F.cross_entropy(pred, emo_labels_labeled)
                else:
                    adv_loss = torch.tensor(0.0, device=device)

                # ---------- preserve loss: keep s_out similar to original s ----------
                cos = F.cosine_similarity(s_hat, s_orig, dim=-1)  # (B,)
                preserve_loss = 1.0 - cos.mean()
                embedding = s_hat.unsqueeze(1)  # (B,1,D)
            else:
                embedding = embedding.unsqueeze(1)
                adv_loss = torch.tensor(0.0, device=device)
                preserve_loss = torch.tensor(0.0, device=device)

        else:
            embedding = None

        # 1. encode text_token
        text_token_emb = self.llm.model.model.embed_tokens(text_token)

        # 2. encode speech_token
        speech_token_emb = self.speech_embedding(speech_token)

        # 3. prepare llm_input/target
        lm_target, lm_input, lm_input_len = self.prepare_lm_input_target(
            text_token, text_token_emb, text_token_len, speech_token,
            speech_token_emb, speech_token_len, embedding=embedding)
        lm_target = lm_target.to(device)

        # 4. run lm forward
        lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
        logits = self.llm_decoder(lm_output)
        loss = self.criterion_ce(logits, lm_target.to(device))
        acc = th_accuracy(logits.view(-1, self.speech_token_size + 3), lm_target, ignore_label=IGNORE_ID)

        loss += self.preserve_weight * preserve_loss + self.adv_weight * adv_loss
        return {'loss': loss, 'acc': acc, 'preserve_loss': preserve_loss, 'adv_loss': adv_loss}

    def forward_dpo(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        text_token = batch['text_token'].to(device)
        text_token_len = batch['text_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        reject_speech_token = batch['reject_speech_token'].to(device)
        reject_speech_token_len = batch['reject_speech_token_len'].to(device)

        # 1. encode text_token
        text_token_emb = self.llm.model.model.embed_tokens(text_token)

        # 2. encode speech_token
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)
        reject_speech_token = unpad_sequence(reject_speech_token, reject_speech_token_len.cpu(), batch_first=True)
        speech_token_combined = speech_token + reject_speech_token
        speech_token_combined = pad_sequence(speech_token_combined, batch_first=True, padding_value=0)
        speech_token_combined_len = torch.concat([speech_token_len, reject_speech_token_len], dim=0)
        speech_token_combined_emb = self.speech_embedding(speech_token_combined)

        # 3. prepare llm_input/target
        lm_target, lm_input, lm_input_len = self.prepare_lm_input_target(text_token.repeat(2, 1), text_token_emb.repeat(2, 1, 1), text_token_len.repeat(2),
                                                                         speech_token_combined, speech_token_combined_emb, speech_token_combined_len)
        lm_target = lm_target.to(device)

        # 4. run lm forward
        lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))
        logits = self.llm_decoder(lm_output)
        chosen_logits = logits[:text_token.shape[0]]
        rejected_logits = logits[text_token.shape[0]:]
        chosen_lm_target = lm_target[:text_token.shape[0]]
        rejected_lm_target = lm_target[text_token.shape[0]:]
        loss = self.criterion_ce(chosen_logits, chosen_lm_target.to(device))
        acc = th_accuracy(chosen_logits.view(-1, self.speech_token_size + 3), chosen_lm_target, ignore_label=IGNORE_ID)

        # 5. calculate dpo logits
        chosen_lm_mask = chosen_lm_target == IGNORE_ID
        rejected_lm_mask = rejected_lm_target == IGNORE_ID
        chosen_logps = torch.gather(chosen_logits.log_softmax(dim=-1), dim=2, index=chosen_lm_target.masked_fill(chosen_lm_mask, 0).unsqueeze(dim=-1)).squeeze(dim=-1)
        rejected_logps = torch.gather(rejected_logits.log_softmax(dim=-1), dim=2, index=rejected_lm_target.masked_fill(rejected_lm_mask, 0).unsqueeze(dim=-1)).squeeze(dim=-1)
        chosen_logps = (chosen_logps * chosen_lm_mask).mean(dim=-1)
        rejected_logps = (rejected_logps * chosen_lm_mask).mean(dim=-1)
        return {'loss': loss, 'acc': acc, 'chosen_logps': chosen_logps, 'rejected_logps': rejected_logps}

    @torch.inference_mode()
    async def inference(
            self,
            text: Union[torch.Tensor, Tuple],
            text_len: Union[torch.Tensor, Tuple],
            prompt_text: Union[torch.Tensor, Tuple],
            prompt_text_len: Union[torch.Tensor, Tuple],
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
            uuid: str = '',
            emotion_lab=[-1, ],
            loracfg=None,
    ) -> Generator[torch.Tensor, None, None]:
        if isinstance(text, Tuple):
            text, pho = text  # here we only use text, the phoneme is not used
            text_len, pho_len = text_len
            prompt_text, prompt_pho = prompt_text
            prompt_text_len, prompt_pho_len = prompt_text_len

        device = text.device
        emotion_lab_tensor = torch.tensor(emotion_lab, dtype=torch.long, device=device)
        if self.non_emotional_label == 0:
            emotion_lab_tensor[emotion_lab_tensor < 0] = 0

        if self.use_embedding:
            embedding = embedding
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            labeled_mask = (emotion_lab_tensor >= 0)
            if self.emotion_num > 0 and labeled_mask.any():
                s_hat = embedding.clone()
                s_hat[labeled_mask] = self.spk_adapter(s_hat[labeled_mask])
                embedding = s_hat.unsqueeze(1)  # (B,1,D)
            else:
                embedding = embedding.unsqueeze(1)
        else:
            embedding = None

        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        text = self.llm.model.model.embed_tokens(text)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)

        if self.use_embedding:
            lm_input = torch.concat([sos_eos_emb, embedding, text, task_id_emb, prompt_speech_token_emb], dim=1)
        else:
            lm_input = torch.concat([sos_eos_emb, text, task_id_emb, prompt_speech_token_emb], dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        async for token in self.inference_wrapper(lm_input, sampling, min_len, max_len, uuid, loracfg):
            yield token

    @torch.inference_mode()
    async def inference_wrapper(self, lm_input, sampling, min_len, max_len, uuid, loracfg):
        if self.use_vllm:
            from vllm.lora.request import LoRARequest
            lora_request = LoRARequest(**loracfg) if loracfg != None else None
            from vllm import SamplingParams, RequestOutput
            sampling_params = SamplingParams(
                min_p=self.vllm_sample_params['min_p'],
                top_k=self.vllm_sample_params['top_k'],  #
                top_p=self.vllm_sample_params['top_p'],  # 默认1.0
                temperature=self.vllm_sample_params['temperature'],  # 默认1.0
                repetition_penalty=self.vllm_sample_params['repetition_penalty'],  # 默认1.0
                stop_token_ids=self.stop_token_ids,
                min_tokens=min_len,
                max_tokens=max_len)

            async for output in self.vllm.generate(
                    {
                        "prompt_embeds": lm_input.squeeze(
                            0).to(torch.bfloat16).to(lm_input.device),
                    },
                    sampling_params=sampling_params,
                    request_id=uuid or f"{time.time()}",
                    lora_request=lora_request,
            ):
                # top_id = output.outputs[0]
                top_id = list(output.outputs[0].token_ids)[-1]
                finished = output.finished

                if top_id in self.stop_token_ids:
                    break
                elif top_id > self.speech_token_size:
                    logger.warning(f"================big token！！！{top_id}")
                    continue
                # in stream mode, yield token one by one
                yield top_id

                if finished:
                    break

        else:
            out_tokens = []
            cache = None
            for i in range(max_len):
                y_pred, cache = self.llm.forward_one_step(lm_input,
                                                          masks=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]), device=lm_input.device)).to(torch.bool),
                                                          cache=cache)
                logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
                top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()
                if top_ids == self.speech_token_size:
                    break
                if top_ids > self.speech_token_size:
                    continue
                elif top_ids > self.speech_token_size:
                    logger.warning(f"================big token！！！{top_ids}")
                    continue
                # in stream mode, yield token one by one
                yield top_ids
                out_tokens.append(top_ids)
                lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)

    @torch.inference_mode()
    def inference_bistream(
            self,
            text: Generator,
            prompt_text: torch.Tensor,
            prompt_text_len: torch.Tensor,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
    ) -> Generator[torch.Tensor, None, None]:

        device = prompt_text.device
        # 1. prepare input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size, dtype=prompt_text.dtype).to(device)
        lm_input = torch.concat([sos_eos_emb], dim=1)

        # 2. iterate text
        out_tokens = []
        cache = None
        # NOTE init prompt_text as text_cache as it is basically impossible prompt_speech_token/prompt_text < 15/5
        text_cache = self.llm.model.model.embed_tokens(prompt_text)
        next_fill_index = -1
        for this_text in text:
            text_cache = torch.concat([text_cache, self.llm.model.model.embed_tokens(this_text)], dim=1)
            # prompt_speech_token_emb not empty, try append to lm_input
            while prompt_speech_token_emb.size(1) != 0:
                if text_cache.size(1) >= self.mix_ratio[0]:
                    lm_input_text, lm_input_speech = text_cache[:, :self.mix_ratio[0]], prompt_speech_token_emb[:, :self.mix_ratio[1]]
                    logging.info('append {} text token {} speech token'.format(lm_input_text.size(1), lm_input_speech.size(1)))
                    lm_input = torch.concat([lm_input, lm_input_text, lm_input_speech], dim=1)
                    text_cache, prompt_speech_token_emb = text_cache[:, self.mix_ratio[0]:], prompt_speech_token_emb[:, self.mix_ratio[1]:]
                else:
                    logging.info('not enough text token to decode, wait for more')
                    break
            # no prompt_speech_token_emb remain, can decode some speech token
            if prompt_speech_token_emb.size(1) == 0:
                if (len(out_tokens) != 0 and out_tokens[-1] == self.speech_token_size + 2) or (len(out_tokens) == 0 and lm_input.size(1) == 1):
                    logging.info('get fill token, need to append more text token')
                    if text_cache.size(1) >= self.mix_ratio[0]:
                        lm_input_text = text_cache[:, :self.mix_ratio[0]]
                        logging.info('append {} text token'.format(lm_input_text.size(1)))
                        if len(out_tokens) != 0 and out_tokens[-1] == self.speech_token_size + 2:
                            lm_input = lm_input_text
                        else:
                            lm_input = torch.concat([lm_input, lm_input_text], dim=1)
                        text_cache = text_cache[:, self.mix_ratio[0]:]
                    else:
                        logging.info('not enough text token to decode, wait for more')
                        continue
                while True:
                    seq_len = lm_input.shape[1] if cache is None else lm_input.shape[1] + cache[0][0].size(2)
                    y_pred, cache = self.llm.forward_one_step(lm_input,
                                                              masks=torch.tril(torch.ones((1, seq_len, seq_len), device=lm_input.device)).to(torch.bool),
                                                              cache=cache)
                    logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
                    if next_fill_index != -1 and len(out_tokens) == next_fill_index:
                        top_ids = self.speech_token_size + 2
                        next_fill_index += (self.mix_ratio[1] + 1)
                    else:
                        top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True).item()
                    if top_ids == self.speech_token_size + 2:
                        next_fill_index = len(out_tokens) + self.mix_ratio[1] + 1
                        logging.info('fill_token index {} next fill_token index {}'.format(len(out_tokens), next_fill_index))
                    out_tokens.append(top_ids)
                    if top_ids >= self.speech_token_size:
                        if top_ids == self.speech_token_size + 2:
                            break
                        else:
                            raise ValueError('should not get token {}'.format(top_ids))
                    yield top_ids
                    lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)

        # 3. final decode
        lm_input = torch.concat([lm_input, text_cache, task_id_emb], dim=1)
        logging.info('no more text token, decode until met eos')
        while True:
            seq_len = lm_input.shape[1] if cache is None else lm_input.shape[1] + cache[0][0].size(2)
            y_pred, cache = self.llm.forward_one_step(lm_input,
                                                      masks=torch.tril(torch.ones((1, seq_len, seq_len), device=lm_input.device)).to(torch.bool),
                                                      cache=cache)
            logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
            top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=False).item()
            out_tokens.append(top_ids)
            if top_ids >= self.speech_token_size:
                if top_ids == self.speech_token_size:
                    break
                else:
                    raise ValueError('should not get token {}'.format(top_ids))
            # in stream mode, yield token one by one
            yield top_ids
            lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)


class Qwen2LM_Phoneme_MultiCode(torch.nn.Module):
    '''
    use sec-attention to fuse Text Embedding and Phoneme embedding.
    the sequence used to predict is Phoneme
    use 40Hz * 6 codec， with delay pattern to re-organize the codec.
    '''

    def __init__(
            self,
            text_encoder_input_size: int,
            llm_input_size: int,
            llm_output_size: int,
            text_token_size: int,
            text_token_dim: int,
            text_tone_size: int,
            text_tone_dim: int,
            text_lang_size: int,
            text_lang_dim: int,
            text_prsd_size: int,
            text_prsd_dim: int,
            speech_token_size: int,
            text_encoder: torch.nn.Module,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            spk_embed_dim: int = 512,
            codebooknum: int = 6,
            src_attn_layers: int = 4,
            use_frontend_prsd: bool = False,
            use_pause_label: bool = False,
    ):
        super().__init__()
        self.llm_input_size = llm_input_size
        self.llm_output_size = llm_output_size
        self.speech_token_size = speech_token_size

        # 0. affine the speaker vector into llm_input_size
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, llm_input_size)

        # 1. build phoneme token inputs related modules
        assert (text_token_dim + text_tone_dim + text_lang_dim + text_prsd_dim) == text_encoder_input_size
        self.text_embedding = nn.ModuleList([
            torch.nn.Embedding(text_token_size, text_token_dim),
            torch.nn.Embedding(text_tone_size, text_tone_dim),
            torch.nn.Embedding(text_lang_size, text_lang_dim),
            torch.nn.Embedding(text_prsd_size, text_prsd_dim)
        ])
        self.use_frontend_prsd = use_frontend_prsd
        self.use_pause_label = use_pause_label
        logger.info(f"llm use frontend prosody: {use_frontend_prsd}, use pause label: {use_pause_label}")

        self.text_encoder = text_encoder
        self.text_encoder_affine_layer = nn.Linear(self.text_encoder.output_size(), llm_input_size)
        #  Hard code Decoder layer as arc-attention
        self.src_attention = torch.nn.ModuleList([
            DecoderLayer(
                llm_input_size,
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                PositionwiseFeedForward(llm_input_size, 4096, 0.1),
                dropout_rate=0.1,
                normalize_before=True,
            ) for _ in range(src_attn_layers)
        ])

        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.fill_token = 2
        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)

        self.codebooknum = codebooknum
        self.eosid = speech_token_size
        self.bosid = speech_token_size + 1
        self.llm = llm
        self.llm_decoder = ARDecoder(self.llm_output_size, self.codebooknum, self.bosid + 1)

        # 3. [Optional] build speech token related modules
        self.speech_embedding = nn.ModuleList([torch.nn.Embedding(
            num_embeddings=self.bosid+1, embedding_dim=self.llm_input_size) for _ in range(self.codebooknum)])
        self.topkacc = MulticlassAccuracy(self.bosid + 1, top_k=5, average="micro", )
        self.criterion_ce = FocalLoss(gamma=1)

        # 4. sampling method
        self.sampling = sampling


    def encode(
            self,
            text: torch.Tensor,
            text_lengths: torch.Tensor,
    ):
        encoder_out, encoder_mask = self.text_encoder(text, text_lengths,
                                                      decoding_chunk_size=-1,
                                                      num_decoding_left_chunks=-1)
        encoder_out_lens = encoder_mask.squeeze(1).sum(1)
        encoder_out = self.text_encoder_affine_layer(encoder_out)
        return encoder_out, encoder_out_lens

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Args:
            text: (B, L, D)
            text_lengths: (B,)
            audio: (B, T, N) or (B, T)
            audio_lengths: (B,)
        """
        text_token = batch['text_token'].to(device)
        text_token_len = batch['text_token_len'].to(device)
        pho_token = batch['pho_token'].to(device)
        pho_token_len = batch['pho_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        embedding = batch['embedding'].to(device)
        bs = embedding.shape[0]

        # 0. prepare llm_target, add bos and eos(optional), then use delay pattern
        vqs = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)
        vqs_bos_eos = []
        for vq in vqs:
            vq_eos = F.pad(vq, (0, 0, 0, 1), value=self.eosid)
            vq_bos_eos = F.pad(vq_eos, (0, 0, 1, 0), value=self.bosid)
            vqs_bos_eos.append(vq_bos_eos)
        speech_token_len = speech_token_len + 2
        speech_token = pad_sequence(vqs_bos_eos, batch_first=True, padding_value=self.eosid)
        # speech_token = pad_sequence(vqs, batch_first=True, padding_value=self.eosid)

        speech_token_delay = get_delay_pattern_codec(speech_token, self.bosid, self.eosid)
        speech_token_delay_len = speech_token_len + self.codebooknum - 1
        speech_token_delay_mask = ~make_pad_mask(
            speech_token_delay_len, speech_token_delay.size(1))

        # 最终预测目标codec, 为delay之后的codec+eos, 而输入，则是taskid+delay之后的codec
        lm_target = F.pad(speech_token_delay, (0,0,0,1), value=self.eosid).long()
        lm_target_mask = F.pad(speech_token_delay_mask, (0,1), value=False)

        # 1. encode phoneme and text
        pho_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](pho_token[:, :, i])
            if not self.use_frontend_prsd and i == 3:
                embed *= 0.0
            pho_embed_list.append(embed)
        pho_token = torch.cat(pho_embed_list, dim=-1)
        pho_token, pho_token_len = self.encode(pho_token, pho_token_len)

        text_token = self.llm.model.model.embed_tokens(text_token)
        text_mask = ~make_pad_mask(text_token_len,
                                   text_token.size(1)).unsqueeze(
            1)  # (B, 1, T1)
        pho_mask = ~make_pad_mask(pho_token_len, pho_token.size(1)).unsqueeze(
            1)  # (B, 1, T2)
        for src_attention in self.src_attention:
            pho_token, pho_mask, text_token, text_mask = src_attention(
                pho_token, pho_mask, text_token, text_mask)

        # 2. embedding projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)
        embedding = embedding.unsqueeze(1)

        # 3. eos and task_id
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)

        # 4. encode speech_token
        speech_token_emb = 0
        for i in range(self.codebooknum):
            vqemb = self.speech_embedding[i](speech_token_delay[..., i])
            speech_token_emb += vqemb

        # 5. cat input sequence
        lm_input = torch.cat([sos_eos_emb.expand(bs, -1, -1), embedding, pho_token,
                task_id_emb.expand(bs, -1, -1), speech_token_emb],dim=1)
        mask_spe = torch.ones(bs, 1, dtype=torch.bool, device=device)
        llm_input_mask = torch.cat([mask_spe, mask_spe, pho_mask.squeeze(1),
                mask_spe, speech_token_delay_mask],dim=1).unsqueeze(1)

        # 6. run lm forward
        lm_output, lm_output_mask = self.llm.forward_one_step(
            lm_input, llm_input_mask.to(device))

        input_len = pho_token.shape[1] + 2   # sos_eos_emb+embedding+pho_token
        speech_token_out = lm_output[:, input_len:]  # 取出对应speech_token的输出部分

        logits = self.llm_decoder(speech_token_out)   # [B,codebook_size,t,num_codebook]
        loss = self.criterion_ce(logits, lm_target, lm_target_mask)
        acc = self.topkacc(logits, lm_target)
        return {'loss': loss, 'acc': acc}

    def sampling_ids(
            self,
            weighted_scores: torch.Tensor,
            decoded_tokens: List,
            sampling: int,
            ignore_eos: bool = True,
    ):
        num_trials, max_trials = 0, 100
        while True:
            top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
            if (not ignore_eos) or (self.speech_token_size not in top_ids):
                break
            num_trials += 1
            if num_trials > max_trials:
                raise RuntimeError(
                    'sampling reaches max_trials {} and still get eos when ignore_eos is True, check your input!'.format(
                        max_trials))
        return top_ids

    @torch.inference_mode()
    def inference(
            self,
            text: Tuple,
            text_len: Tuple,
            prompt_text: Tuple,
            prompt_text_len: Tuple,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
    ) -> Generator[torch.Tensor, None, None]:
        device = embedding.device

        text, pho = text
        text_len, pho_len = text_len
        prompt_text, prompt_pho = prompt_text
        prompt_text_len, prompt_pho_len = prompt_text_len

        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        pho = torch.concat([prompt_pho, pho], dim=1)
        pho_len += prompt_pho_len

        pho_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](pho[:, :, i])
            if not self.use_frontend_prsd and i == 3:
                embed *= 0.0
            pho_embed_list.append(embed)
        pho = torch.cat(pho_embed_list, dim=-1)

        # 1. encode text
        pho, pho_len = self.encode(pho, pho_len)
        text = self.llm.model.model.embed_tokens(text)

        text_mask = ~make_pad_mask(text_len, text.size(1)).unsqueeze(
            1)  # (B, 1, T1)
        pho_mask = ~make_pad_mask(pho_len, pho.size(1)).unsqueeze(
            1)  # (B, 1, T2)
        for src_attention in self.src_attention:
            pho, pho_mask, text, text_mask = src_attention(pho, pho_mask, text,
                                                           text_mask)

        # 2. encode embedding
        if embedding.shape[0] != 0:
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            embedding = embedding.unsqueeze(dim=1)
        else:
            embedding = torch.zeros(1, 0, self.llm_input_size,
                                    dtype=text.dtype).to(device)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = 0
            for i in range(self.codebooknum):
                vqemb = self.speech_embedding[i](prompt_speech_token[..., i])
                prompt_speech_token_emb += vqemb
            # prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size,
                                                  dtype=text.dtype).to(device)
        lm_input = torch.concat(
            [sos_eos_emb, embedding, pho, task_id_emb, prompt_speech_token_emb],
            dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        leftpoint = 1  # 去除bos
        chunksize = 1
        winsize = chunksize + self.codebooknum - 1
        out_tokens = torch.full((1,0,self.codebooknum), self.bosid, dtype=torch.long, device=device)
        cache = None
        for i in range(max_len):
            y_pred, cache = self.llm.forward_one_step(
                lm_input,
                masks=torch.ones((1, lm_input.shape[1], lm_input.shape[1]),
                                 device=lm_input.device).to(torch.bool),
                cache=cache)

            logit_all = y_pred[:, -1:, :]
            curlogit = self.llm_decoder(logit_all)
            nexttokens = self.sampling(curlogit.squeeze().transpose(0, 1), out_tokens)

            if (nexttokens == self.eosid).all():
                returncodes = out_tokens[:,leftpoint:leftpoint+winsize]
                yield revert_delay_pattern_codec(returncodes)
                break

            out_tokens = torch.cat([out_tokens, nexttokens[None,None,:]],dim=1)
            if out_tokens.shape[1] == leftpoint + winsize:
                returncodes = out_tokens[:,leftpoint:leftpoint+winsize]
                leftpoint += chunksize
                yield revert_delay_pattern_codec(returncodes)
            lm_input = torch.zeros((1, 1, 896), device=device, dtype=y_pred.dtype)
            for idx, nexttoken in enumerate(nexttokens):
                lm_input += self.speech_embedding[idx](nexttoken)


class Qwen2LM_Phoneme_Sglang(torch.nn.Module):
    '''
    use sec-attention to fuse Text Embedding and Phoneme embedding.
    the sequence used to predict is Phoneme
    '''

    def __init__(
            self,
            text_encoder_input_size: int,
            llm_input_size: int,
            llm_output_size: int,
            text_token_size: int,
            text_token_dim: int,
            text_tone_size: int,
            text_tone_dim: int,
            text_lang_size: int,
            text_lang_dim: int,
            text_prsd_size: int,
            text_prsd_dim: int,
            speech_token_size: int,
            text_encoder: torch.nn.Module,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            spk_embed_dim: int = 512,
            use_frontend_prsd: bool = False,
            use_pause_label: bool = False,
            qwen_sglang_config: dict = None,
    ):
        super().__init__()
        self.llm_input_size = llm_input_size
        self.llm_output_size = llm_output_size
        self.speech_token_size = speech_token_size
        # 1. build phoneme token inputs related modules
        assert (text_token_dim + text_tone_dim + text_lang_dim + text_prsd_dim) == text_encoder_input_size
        self.text_embedding = nn.ModuleList([
            torch.nn.Embedding(text_token_size, text_token_dim),
            torch.nn.Embedding(text_tone_size, text_tone_dim),
            torch.nn.Embedding(text_lang_size, text_lang_dim),
            torch.nn.Embedding(text_prsd_size, text_prsd_dim)
        ])
        self.use_frontend_prsd = use_frontend_prsd
        self.use_pause_label = use_pause_label
        logger.info(
            f"llm use frontend prosody: {use_frontend_prsd}, use pause label: {use_pause_label}")

        self.text_encoder = text_encoder
        self.text_encoder_affine_layer = nn.Linear(
            self.text_encoder.output_size(), llm_input_size
        )
        #  Hard code Decoder layer as arc-attention
        self.src_attention = torch.nn.ModuleList([
            DecoderLayer(
                llm_input_size,
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                PositionwiseFeedForward(llm_input_size, 4096, 0.1),
                dropout_rate=0.1,
                normalize_before=True,
            ) for _ in range(1)
        ])

        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.fill_token = 2

        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)
        self.llm = llm
        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 3)
        self.criterion_ce = LabelSmoothingLoss(
            size=speech_token_size + 3,
            padding_idx=IGNORE_ID,
            smoothing=lsm_weight,
            normalize_length=length_normalized_loss,
        )

        # 3. [Optional] build speech token related modules
        self.speech_embedding = torch.nn.Embedding(speech_token_size + 3,
                                                   llm_input_size)
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim,
                                                      llm_input_size)

        # 4. sampling method
        self.sampling = sampling

        # 5. use_sglang
        self.use_sglang = (qwen_sglang_config is not None)
        if self.use_sglang:
            from sglang.test.test_utils import (
                DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
                DEFAULT_URL_FOR_TEST,
                popen_launch_server,
            )

            # "/data/megastore/SHARE/SHARE_checkpoints_lamtts_svn/acoustics/qwen/forsglang"
            model_path = qwen_sglang_config['model_path']
            self.base_url = qwen_sglang_config['base_url']
            mem_fraction = qwen_sglang_config['mem_ratio']

            python_bin_dir = os.path.dirname(sys.executable)
            custom_env = os.environ.copy()
            custom_env["PATH"] = f"{python_bin_dir}:{custom_env['PATH']}"
            self.sgprocess = popen_launch_server(
                model_path,
                self.base_url,
                env=custom_env,   # 这个决定启动子进程的python环境
                timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
                other_args=[
                    "--disable-radix",  ### 开启输入embedding模式
                    "--skip-tokenizer-init",  ### 开启直接返回token
                    "--random-seed=1234",  ### 做实验debug，可去掉
                    "--base-gpu-id=0",  ### 指定gpu id，可去掉
                    f"--mem-fraction-static={mem_fraction}",
                    ### 控制kvcache占用显存比例，去掉self.llm后可以调大
                    "--dtype=bfloat16",
                    ### float32跑不通，必须downscale成bfloat16，效果区别不大
                ],
            )

            signal.signal(signal.SIGINT, self.signal_handler)
            signal.signal(signal.SIGTERM, self.signal_handler)
            atexit.register(self.cleanup)

            del self.llm.model.model.layers  # 去掉torch原始参数节省显存

    def cleanup(self):
        from sglang.srt.utils import kill_process_tree
        kill_process_tree(self.sgprocess.pid)

    def signal_handler(self, sig, frame):
        self.cleanup()
        sys.exit(0)

    def send_request(self, base_url, payload):
        """Send a POST request to the API and return the response."""
        response = requests.post(
            base_url + "/generate",
            json=payload,
            timeout=30,  # Set a reasonable timeout for the API request
            stream=True,  # 这里也必须开启流式，不然sglang会把整句生成完才开始返回
        )
        if response.status_code == 200:
            return response
        return {
            "error": f"Request failed with status {response.status_code}: {response.text}"
        }

    def encode(
            self,
            text: torch.Tensor,
            text_lengths: torch.Tensor,
    ):
        encoder_out, encoder_mask = self.text_encoder(text, text_lengths,
                                                      decoding_chunk_size=-1,
                                                      num_decoding_left_chunks=-1)
        encoder_out_lens = encoder_mask.squeeze(1).sum(1)
        encoder_out = self.text_encoder_affine_layer(encoder_out)
        return encoder_out, encoder_out_lens

    def pad_unpad_sequence(self, sos_eos_emb, embedding, text_token,
                           text_token_len,
                           task_id_emb, speech_token, speech_token_len):
        text_token = unpad_sequence(text_token, text_token_len.cpu(),
                                    batch_first=True)
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(),
                                      batch_first=True)
        lm_input = [torch.concat(
            [sos_eos_emb.squeeze(dim=0), embedding[i], text_token[i],
             task_id_emb.squeeze(dim=0), speech_token[i]], dim=0)
            for i in range(len(text_token))]
        lm_input_len = torch.tensor([i.size(0) for i in lm_input],
                                    dtype=torch.int32)
        lm_input = pad_sequence(lm_input, batch_first=True,
                                padding_value=IGNORE_ID)
        return lm_input, lm_input_len

    def sampling_ids(
            self,
            weighted_scores: torch.Tensor,
            decoded_tokens: List,
            sampling: int,
            ignore_eos: bool = True,
    ):
        num_trials, max_trials = 0, 100
        while True:
            top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
            if (not ignore_eos) or (self.speech_token_size not in top_ids):
                break
            num_trials += 1
            if num_trials > max_trials:
                logger.warning(f'sampling reaches max_trials {max_trials} and still get eos when ignore_eos is True, check your input!')
                break
        return top_ids

    @torch.inference_mode()
    def inference(
            self,
            text: Tuple,
            text_len: Tuple,
            prompt_text: Tuple,
            prompt_text_len: Tuple,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 25,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
            uuid: str='',
    ) -> Generator[torch.Tensor, None, None]:
        device = embedding.device

        text, pho = text
        text_len, pho_len = text_len
        prompt_text, prompt_pho = prompt_text
        prompt_text_len, prompt_pho_len = prompt_text_len

        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        pho = torch.concat([prompt_pho, pho], dim=1)
        pho_len += prompt_pho_len

        pho_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](pho[:, :, i])
            if not self.use_frontend_prsd and i == 3:
                embed *= 0.0
            pho_embed_list.append(embed)
        pho = torch.cat(pho_embed_list, dim=-1)

        # 1. encode text
        pho, pho_len = self.encode(pho, pho_len)
        text = self.llm.model.model.embed_tokens(text)

        text_mask = ~make_pad_mask(text_len, text.size(1)).unsqueeze(
            1)  # (B, 1, T1)
        pho_mask = ~make_pad_mask(pho_len, pho.size(1)).unsqueeze(
            1)  # (B, 1, T2)
        for src_attention in self.src_attention:
            pho, pho_mask, text, text_mask = src_attention(pho, pho_mask, text,
                                                           text_mask)

        # 2. encode embedding
        if embedding.shape[0] != 0:
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            embedding = embedding.unsqueeze(dim=1)
        else:
            embedding = torch.zeros(1, 0, self.llm_input_size,
                                    dtype=text.dtype).to(device)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size,
                                                  dtype=text.dtype).to(device)
        lm_input = torch.concat(
            [sos_eos_emb, embedding, pho, task_id_emb, prompt_speech_token_emb],
            dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        if self.use_sglang:
            payload = {
                "stream": True,
                "input_embeds": lm_input.squeeze().tolist(),
                "sampling_params": {
                    "stop_token_ids": [self.speech_token_size],
                    "max_new_tokens": max_len,
                    "temperature": 1.0,
                    "top_p": self.sampling.keywords['top_p'],
                    "top_k": self.sampling.keywords['top_k']
                }
            }
            response = self.send_request(self.base_url, payload)
            for chunk in response.iter_lines(decode_unicode=False):
                chunk = chunk.decode("utf-8")
                if chunk and chunk.startswith("data:"):
                    if chunk == "data: [DONE]":
                        break
                    data = json.loads(chunk[5:].strip("\n"))
                    if 'output_ids' in data:
                        top_ids = data["output_ids"][-1]   # 兼容高版本sglang
                    else:
                        top_ids = data["token_ids"][-1]   # sglang==0.4.3.post2

                    if top_ids == self.speech_token_size:
                        break
                    elif top_ids > self.speech_token_size:  # sglang推理时会对输入embedding进行padding，会增加token个数
                        logger.warning(f"================big token！！！{top_ids}")
                        continue

                    yield top_ids
        else:
            out_tokens = []
            cache = None
            for i in range(max_len):
                y_pred, cache = self.llm.forward_one_step(
                    lm_input,
                    # masks=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]), device=lm_input.device)).to(torch.bool),
                    masks=torch.ones((1, lm_input.shape[1], lm_input.shape[1]),
                                     device=lm_input.device).to(torch.bool),
                    cache=cache)
                logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
                top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens,
                                            sampling,
                                            ignore_eos=True if i < min_len else False).item()
                if top_ids == self.speech_token_size:
                    break
                elif top_ids > self.speech_token_size:
                    logger.warning(f"================big token！！！{top_ids}")
                    continue
                # in stream mode, yield token one by one
                yield top_ids
                out_tokens.append(top_ids)
                lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)


class Qwen2LM_Phoneme_Vllm(torch.nn.Module):
    '''
    use sec-attention to fuse Text Embedding and Phoneme embedding.
    the sequence used to predict is Phoneme, use vllm for llm inference
    '''

    def __init__(
            self,
            text_encoder_input_size: int,
            llm_input_size: int,
            llm_output_size: int,
            text_token_size: int,
            text_token_dim: int,
            text_tone_size: int,
            text_tone_dim: int,
            text_lang_size: int,
            text_lang_dim: int,
            text_prsd_size: int,
            text_prsd_dim: int,
            speech_token_size: int,
            text_encoder: torch.nn.Module,
            llm: torch.nn.Module,
            sampling: Callable,
            length_normalized_loss: bool = True,
            lsm_weight: float = 0.0,
            spk_embed_dim: int = 512,
            use_frontend_prsd: bool = False,
            use_pause_label: bool = False,
            qwen_sglang_config: dict = None,
            vllm_sample_params: dict = {
                "top_k":   10,  #
                "top_p":  0.8,  # 默认1.0
                "temperature": 1.0,  # 默认1.0
                "repetition_penalty": 1.0,  # 默认1.0,设置大于一容易丢发音
            },
            emotion_num: int = 0,
            non_emotional_label: int = -1,  # 非多情感数据标签
            add_emotion_before_llm: bool = False,  # 输入llm前是否加上情绪向量
            emotion_fuse_type: str = 'cat',  # add OR cat
    ):
        super().__init__()
        self.llm_input_size = llm_input_size
        self.llm_output_size = llm_output_size
        self.speech_token_size = speech_token_size
        self.text_encoder_input_size = text_encoder_input_size
        # 1. build phoneme token inputs related modules
        assert (text_token_dim + text_tone_dim + text_lang_dim + text_prsd_dim) == text_encoder_input_size
        self.text_embedding = nn.ModuleList([
            torch.nn.Embedding(text_token_size, text_token_dim),
            torch.nn.Embedding(text_tone_size, text_tone_dim),
            torch.nn.Embedding(text_lang_size, text_lang_dim),
            torch.nn.Embedding(text_prsd_size, text_prsd_dim)
        ])
        self.use_frontend_prsd = use_frontend_prsd
        self.use_pause_label = use_pause_label
        self.emotion_num = emotion_num
        self.non_emotional_label = non_emotional_label
        self.add_emotion_before_llm = add_emotion_before_llm
        self.emotion_fuse_type = emotion_fuse_type
        logger.info(
            f"llm use prosody: {use_frontend_prsd}, use pause label: {use_pause_label}, "
            f"emotion_num: {emotion_num}, non_emotional_label: {non_emotional_label}, "
            f"add_emotion_before_llm: {add_emotion_before_llm}, emotion_fuse_type: {emotion_fuse_type}")
        if self.emotion_num > 0:  # emotion_embedding直接放到pho encoder前加入
            self.emotion_embedding = torch.nn.Embedding(self.emotion_num, text_encoder_input_size)
            self.spk_adapter = SpeakerAdapter(dim=llm_input_size,
                                              bottleneck=256)
            num_emotions = max(1, self.emotion_num)  # 避免为0
            self.emo_adversary = nn.Sequential(
                nn.Linear(llm_input_size, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, num_emotions)
            )
        self.adv_weight = 1.0  # 对抗强度
        self.preserve_weight = 1.0  # 保持音色相似
        self.grl_lambda = 1.0  # GRL 系数

        if self.add_emotion_before_llm:
            self.emotion_affine_layer = nn.Linear(text_encoder_input_size, llm_input_size, bias=False)
            # self.emotion_affine_layer = nn.Linear(text_encoder_input_size, llm_input_size)

        self.vllm_sample_params = vllm_sample_params
        logger.info(f"vllm sampling params: {vllm_sample_params}")

        self.text_encoder = text_encoder
        self.text_encoder_affine_layer = nn.Linear(
            self.text_encoder.output_size(), llm_input_size
        )
        #  Hard code Decoder layer as arc-attention
        self.src_attention = torch.nn.ModuleList([
            DecoderLayer(
                llm_input_size,
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                MultiHeadedAttention(16, llm_input_size, 0.1, key_bias=True),
                PositionwiseFeedForward(llm_input_size, 4096, 0.1),
                dropout_rate=0.1,
                normalize_before=True,
            ) for _ in range(1)
        ])

        # 2. build speech token language model related modules
        self.sos_eos = 0
        self.task_id = 1
        self.fill_token = 2

        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)
        self.llm = llm
        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 3)
        self.criterion_ce = LabelSmoothingLoss(
            size=speech_token_size + 3,
            padding_idx=IGNORE_ID,
            smoothing=lsm_weight,
            normalize_length=length_normalized_loss,
        )

        # 3. [Optional] build speech token related modules
        self.speech_embedding = torch.nn.Embedding(speech_token_size + 3,
                                                   llm_input_size)
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim,
                                                      llm_input_size)

        # 4. sampling method
        self.sampling = sampling

        # 5. vllm related
        self.stop_token_ids = [speech_token_size + i for i in range(3)]
        self.vllm_output_queue = {}
        self.use_vllm = (qwen_sglang_config is not None)
        if self.use_vllm and 0==1:  # 在外部单独事件循环中创建vllm
            os.environ["VLLM_ATTENTION_BACKEND"] = "FLASHINFER"  # 默认后端为Flash-Attn, 改为FlashInfer
            python_bin_dir = os.path.dirname(sys.executable)
            custom_env = os.environ.copy()
            custom_env["PATH"] = f"{python_bin_dir}:{custom_env['PATH']}"
            model_path = qwen_sglang_config['model_path']
            self.base_url = qwen_sglang_config['base_url']  # 直接在同一个进程中启动，此参数不需要了
            mem_fraction = qwen_sglang_config['mem_ratio']

            from vllm import ModelRegistry
            from cosyvoice.vllm.cosyvoice2 import CosyVoice2ForCausalLM
            ModelRegistry.register_model("CosyVoice2ForCausalLM", CosyVoice2ForCausalLM)
            # from vllm import EngineArgs, LLMEngine   # 同步
            from vllm import AsyncLLMEngine as LLMEngine  # 异步
            from vllm.engine.arg_utils import AsyncEngineArgs as EngineArgs

            engine_args = EngineArgs(model=model_path,
                                     skip_tokenizer_init=True,
                                     enable_prompt_embeds=True,
                                     disable_custom_all_reduce=True,
                                     gpu_memory_utilization=float(mem_fraction))
            self.vllm = LLMEngine.from_engine_args(engine_args)
            del self.llm.model.model.layers

    def encode(
            self,
            text: torch.Tensor,
            text_lengths: torch.Tensor,
    ):
        encoder_out, encoder_mask = self.text_encoder(text, text_lengths,
                                                      decoding_chunk_size=-1,
                                                      num_decoding_left_chunks=-1)
        encoder_out_lens = encoder_mask.squeeze(1).sum(1)
        encoder_out = self.text_encoder_affine_layer(encoder_out)
        return encoder_out, encoder_out_lens

    def pad_unpad_sequence(self, sos_eos_emb, embedding, text_token,
                           text_token_len,
                           task_id_emb, speech_token, speech_token_len):
        text_token = unpad_sequence(text_token, text_token_len.cpu(),
                                    batch_first=True)
        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(),
                                      batch_first=True)
        lm_input = [torch.concat(
            [sos_eos_emb.squeeze(dim=0), embedding[i], text_token[i],
             task_id_emb.squeeze(dim=0), speech_token[i]], dim=0)
            for i in range(len(text_token))]
        lm_input_len = torch.tensor([i.size(0) for i in lm_input],
                                    dtype=torch.int32)
        lm_input = pad_sequence(lm_input, batch_first=True,
                                padding_value=IGNORE_ID)
        return lm_input, lm_input_len

    def sampling_ids(
            self,
            weighted_scores: torch.Tensor,
            decoded_tokens: List,
            sampling: int,
            ignore_eos: bool = True,
    ):
        num_trials, max_trials = 0, 100
        while True:
            top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)
            if (not ignore_eos) or (self.speech_token_size not in top_ids):
                break
            num_trials += 1
            if num_trials > max_trials:
                logger.warning(f'sampling reaches max_trials {max_trials} and still get eos when ignore_eos is True, check your input!')
                break
        return top_ids

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Args:
            text: (B, L, D)
            text_lengths: (B,)
            audio: (B, T, N) or (B, T)
            audio_lengths: (B,)
        """
        text_token = batch['text_token'].to(device)
        text_token_len = batch['text_token_len'].to(device)
        pho_token = batch['pho_token'].to(device)
        pho_token_len = batch['pho_token_len'].to(device)
        speech_token = batch['speech_token'].to(device)
        speech_token_len = batch['speech_token_len'].to(device)
        embedding = batch['embedding'].to(device)
        emotion_lab = batch['emos']
        emotion_lab_tensor = torch.tensor(batch['emos'], dtype=torch.long, device=device)

        # 0. prepare llm_target
        if self.add_emotion_before_llm and self.emotion_fuse_type == 'cat':
            extra_token_num = 4   # 包含情绪token
        else:
            extra_token_num = 2   # spk_embeding, task_id
        lm_target = [torch.tensor(
            [IGNORE_ID] * (extra_token_num + pho_token_len[i]) +
              speech_token[i,:speech_token_len[i]].tolist() +
                [self.speech_token_size]) for i in range(text_token.size(0))]
        lm_target = pad_sequence(lm_target, batch_first=True,
                                 padding_value=IGNORE_ID).to(device)

        # 1. encode phoneme and text
        pho_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](pho_token[:, :, i])
            if not self.use_frontend_prsd and i == 3:
                embed *= 0.0
            pho_embed_list.append(embed)
        pho_token = torch.cat(pho_embed_list, dim=-1)
        # pho_token add emotion embedding
        if self.emotion_num > 0:
            emotion_emb_list = []
            for idx, lab in enumerate(emotion_lab):
                if self.non_emotional_label == 0 and lab == -1:
                    lab = 0  # 将没有情绪标签的数据，也定义为中性
                    emotion_lab_tensor[idx] = 0
                if lab < 0:
                    emotion_emb_list.append(
                        torch.zeros(self.text_encoder_input_size).reshape(1, 1, -1).to(device))
                else:
                    emotion_emb_list.append(
                        self.emotion_embedding(torch.LongTensor([lab]).to(device)).reshape(1, 1, -1))
            emotion_emb = torch.cat(emotion_emb_list, dim=0)  # B 1 D1
            if self.emotion_fuse_type == 'add':
                pho_token += emotion_emb  # B L D

        pho_token, pho_token_len = self.encode(pho_token, pho_token_len)

        text_token = self.llm.model.model.embed_tokens(text_token)
        text_mask = ~make_pad_mask(text_token_len, text_token.size(1)).unsqueeze(1)  # (B, 1, T1)
        pho_mask = ~make_pad_mask(pho_token_len, pho_token.size(1)).unsqueeze(1)  # (B, 1, T2)
        for src_attention in self.src_attention:
            pho_token, pho_mask, text_token, text_mask = src_attention(
                pho_token, pho_mask, text_token, text_mask)

        # 2. embedding projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)

        if self.emotion_num > 0 and self.add_emotion_before_llm:
            emotion_token = self.emotion_affine_layer(emotion_emb)  # B 1 D
            if self.emotion_fuse_type == 'add':
                pho_token += emotion_token
            else:
                pho_token = torch.cat([emotion_token, pho_token, emotion_token], dim=1)
                pho_token_len += 2

            s = embedding  # (B, D)
            s_orig = s.detach()  # detach original as reference (no grad needed)
            s_hat = s.clone()

            # ---------- adversarial loss: only on labeled samples ----------
            labeled_mask = (emotion_lab_tensor >= 0)  # (B,)
            if labeled_mask.any():
                # only update the embedding with emotion label
                s_hat[labeled_mask] = self.spk_adapter(s[labeled_mask])  # (B_lab, D)
                s_out_labeled = s_hat[labeled_mask]  # (B_lab, D)
                # apply GRL before classifier
                s_adv = grad_reverse(s_out_labeled, self.grl_lambda)
                pred = self.emo_adversary(s_adv)  # (B_lab, num_emotions)

                emo_labels_labeled = emotion_lab_tensor[labeled_mask]  # (B_lab,)
                adv_loss = F.cross_entropy(pred, emo_labels_labeled)
            else:
                adv_loss = torch.tensor(0.0, device=device)

            # ---------- preserve loss: keep s_out similar to original s ----------
            # use cosine similarity: we want high similarity -> loss = 1 - cos_mean
            cos = F.cosine_similarity(s_hat, s_orig, dim=-1)  # (B,)
            preserve_loss = 1.0 - cos.mean()

            # ---------- now continue pipeline: unsqueeze s_out and compose lm_input ----------
            embedding = s_hat.unsqueeze(1)  # (B,1,D)

        else:
            embedding = embedding.unsqueeze(1)
            adv_loss = torch.tensor(0.0, device=device)
            preserve_loss = torch.tensor(0.0, device=device)

        # 3. eos and task_id
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)

        # 4. encode speech_token
        speech_token = self.speech_embedding(speech_token)

        # 5. unpad and pad
        lm_input, lm_input_len = self.pad_unpad_sequence(
            sos_eos_emb, embedding, pho_token, pho_token_len,
            task_id_emb, speech_token, speech_token_len)

        # 6. run lm forward
        # llm_input_mask = torch.tril(torch.ones((
        #     lm_input.size(0), lm_input.size(1), lm_input.size(1)),
        #     device=lm_input.device)).to(torch.bool)   # B T T 三角矩阵，只attention前文
        llm_input_mask = ~make_pad_mask(lm_input_len, lm_input.size(1)).unsqueeze(1)  # (B, 1, T)
        # 目前训练时使用全局attention, 不设置动态chunk和固定chunk
        llm_input_mask = add_optional_chunk_mask(
            lm_input, llm_input_mask, use_dynamic_chunk=False,
            use_dynamic_left_chunk=False, decoding_chunk_size=-1,
            static_chunk_size=-1, num_decoding_left_chunks=-1)    # B, T, T

        lm_output, lm_output_mask = self.llm.forward_one_step(lm_input, llm_input_mask.to(device))
        logits = self.llm_decoder(lm_output)

        main_loss = self.criterion_ce(logits, lm_target)
        acc = th_accuracy(logits.view(-1, self.speech_token_size + 3),
                          lm_target, ignore_label=IGNORE_ID)

        # ---------- total loss composition ----------
        loss = main_loss \
               + self.preserve_weight * preserve_loss \
               + self.adv_weight * adv_loss

        return {'loss': loss, 'acc': acc, 'preserve_loss': preserve_loss,
                'adv_loss': adv_loss, }


    @torch.inference_mode()
    async def inference(
            self,
            text: Tuple,
            text_len: Tuple,
            prompt_text: Tuple,
            prompt_text_len: Tuple,
            prompt_speech_token: torch.Tensor,
            prompt_speech_token_len: torch.Tensor,
            embedding: torch.Tensor,
            sampling: int = 20,
            max_token_text_ratio: float = 20,
            min_token_text_ratio: float = 2,
            uuid: str = "",
            emotion_lab: list = [-1, ],
            loracfg=None,
    ) -> Generator[torch.Tensor, None, None]:
        device = embedding.device
        emotion_lab_tensor = torch.tensor(emotion_lab, dtype=torch.long, device=device)

        text, pho = text
        text_len, pho_len = text_len
        prompt_text, prompt_pho = prompt_text
        prompt_text_len, prompt_pho_len = prompt_text_len

        text = torch.concat([prompt_text, text], dim=1)
        text_len += prompt_text_len
        pho = torch.concat([prompt_pho, pho], dim=1)
        pho_len += prompt_pho_len

        pho_embed_list = []
        for i in range(len(self.text_embedding)):
            embed = self.text_embedding[i](pho[:, :, i])
            if not self.use_frontend_prsd and i == 3:
                embed *= 0.0
            pho_embed_list.append(embed)
        pho = torch.cat(pho_embed_list, dim=-1)

        # pho_token add emotion embedding
        if self.emotion_num > 0:
            emotion_emb_list = []
            for idx, lab in enumerate(emotion_lab):
                if self.non_emotional_label == 0 and lab == -1:
                    lab = 0  # 将没有情绪标签的数据，也定义为中性
                    emotion_lab_tensor[idx] = 0
                if lab < 0:
                    emotion_emb_list.append(
                        torch.zeros(self.text_encoder_input_size).reshape(1, 1, -1).to(device))
                else:
                    emotion_emb_list.append(
                        self.emotion_embedding(torch.LongTensor([lab]).to(device)).reshape(1, 1, -1))

            emotion_emb = torch.cat(emotion_emb_list, dim=0)  # B 1 D
            if self.emotion_fuse_type == 'add':
                pho += emotion_emb

        # 1. encode text
        pho, pho_len = self.encode(pho, pho_len)
        text = self.llm.model.model.embed_tokens(text)

        text_mask = ~make_pad_mask(text_len, text.size(1)).unsqueeze(
            1)  # (B, 1, T1)
        pho_mask = ~make_pad_mask(pho_len, pho.size(1)).unsqueeze(
            1)  # (B, 1, T2)
        for src_attention in self.src_attention:
            pho, pho_mask, text, text_mask = src_attention(pho, pho_mask, text,
                                                           text_mask)
        if self.emotion_num > 0 and self.add_emotion_before_llm:
            emotion_token = self.emotion_affine_layer(emotion_emb)  # B 1 D
            if self.emotion_fuse_type == 'add':
                pho += emotion_token
            else:
                pho = torch.cat([emotion_token, pho, emotion_token], dim=1)  # BLD

        # 2. encode embedding
        if embedding.shape[0] != 0:
            embedding = F.normalize(embedding, dim=1)
            embedding = self.spk_embed_affine_layer(embedding)
            labeled_mask = (emotion_lab_tensor >= 0)  # (B,)
            if self.emotion_num > 0 and labeled_mask.any():  # 只有非-1的emotion才会修改说话人向量
                s_hat = embedding.clone()
                s_hat[labeled_mask] = self.spk_adapter(s_hat[labeled_mask])
                embedding = s_hat.unsqueeze(1)  # (B,1,D)
            else:
                embedding = embedding.unsqueeze(1)
        else:
            embedding = torch.zeros(1, 0, self.llm_input_size,
                                    dtype=text.dtype).to(device)

        # 3. concat llm_input
        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)
        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)
        if prompt_speech_token_len != 0:
            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)
        else:
            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size,
                                                  dtype=text.dtype).to(device)
        lm_input = torch.concat(
            [sos_eos_emb, embedding, pho, task_id_emb, prompt_speech_token_emb],
            dim=1)

        # 4. cal min/max_length
        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)
        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)

        # 5. step by step decode
        if self.use_vllm:
            from vllm.lora.request import LoRARequest
            lora_request = LoRARequest(**loracfg) if loracfg != None else None
            from vllm import SamplingParams
            sampling_params = SamplingParams(
                min_p=self.vllm_sample_params['min_p'],
                top_k=self.vllm_sample_params['top_k'],  #
                top_p=self.vllm_sample_params['top_p'],  # 默认1.0
                temperature=self.vllm_sample_params['temperature'],  # 默认1.0
                repetition_penalty=self.vllm_sample_params['repetition_penalty'],  # 默认1.0
                stop_token_ids=self.stop_token_ids,
                min_tokens=min_len,
                max_tokens=max_len)

            async for output in self.vllm.generate(
                    {
                        "prompt_embeds": lm_input.squeeze(
                            0).to(torch.bfloat16).to(lm_input.device),
                    },
                    sampling_params=sampling_params,
                    request_id=uuid or f"{time.time()}",
                    lora_request=lora_request,
            ):
                # top_id = output.outputs[0]
                top_id = list(output.outputs[0].token_ids)[-1]
                finished = output.finished

                if top_id in self.stop_token_ids:
                    break
                elif top_id > self.speech_token_size:
                    logger.warning(f"================big token！！！{top_id}")
                    continue
                # in stream mode, yield token one by one
                yield top_id

                if finished:
                    break

        else:
            out_tokens = []
            cache = None
            for i in range(max_len):
                y_pred, cache = self.llm.forward_one_step(
                    lm_input,
                    # masks=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]), device=lm_input.device)).to(torch.bool),
                    masks=torch.ones((1, lm_input.shape[1], lm_input.shape[1]),
                                     device=lm_input.device).to(torch.bool),
                    cache=cache)
                logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)
                top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens,
                                            sampling,
                                            ignore_eos=True if i < min_len else False).item()
                if top_ids == self.speech_token_size:
                    break
                elif top_ids > self.speech_token_size:
                    logger.warning(f"================big token！！！{top_ids}")
                    continue
                # in stream mode, yield token one by one
                yield top_ids
                out_tokens.append(top_ids)
                lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)